#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Ph√¢n t√≠ch hi·ªáu su·∫•t ingestion pipeline v√† t·∫°o b√°o c√°o chi ti·∫øt."""

import logging
import os
import sys
from pathlib import Path
from datetime import datetime, timedelta

import psycopg2
from psycopg2.extras import RealDictCursor

# Fix encoding for Windows
if sys.platform == "win32":
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

sys.path.insert(0, str(Path(__file__).parent.parent))

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Load .env
try:
    from dotenv import load_dotenv
    env_path = Path(__file__).parent.parent / ".env"
    if env_path.exists():
        load_dotenv(env_path, override=True)
except ImportError:
    pass


def get_db_connection():
    """Get database connection."""
    database_url = os.getenv("DATABASE_URL")
    if database_url:
        if database_url.startswith("postgresql://") or database_url.startswith("postgres://"):
            url = database_url.replace("postgresql://", "").replace("postgres://", "")
            if "@" in url:
                auth_part, host_part = url.split("@", 1)
                if ":" in auth_part:
                    db_user, db_password = auth_part.split(":", 1)
                else:
                    db_user = auth_part
                    db_password = ""
                
                if "/" in host_part:
                    host_port, db_name = host_part.rsplit("/", 1)
                    if ":" in host_port:
                        db_host, db_port = host_port.split(":", 1)
                    else:
                        db_host = host_port
                        db_port = "5432"
                else:
                    db_host = host_part
                    db_port = "5432"
                    db_name = "postgres"
            
            return psycopg2.connect(
                host=db_host,
                port=db_port,
                database=db_name,
                user=db_user,
                password=db_password
            )
    
    return psycopg2.connect(
        host=os.getenv("DB_HOST", "localhost"),
        port=os.getenv("DB_PORT", "5432"),
        database=os.getenv("DB_NAME", "hanoiair_chatbot"),
        user=os.getenv("DB_USER", "postgres"),
        password=os.getenv("DB_PASSWORD", "")
    )


def analyze_api_calls(db_conn):
    """Ph√¢n t√≠ch s·ªë l∆∞·ª£ng API calls ƒë√£ th·ª±c hi·ªán."""
    print("=" * 80)
    print("PH√ÇN T√çCH API CALLS")
    print("=" * 80)
    
    with db_conn.cursor(cursor_factory=RealDictCursor) as cur:
        # T·ªïng s·ªë requests
        cur.execute("""
            SELECT source_id, COUNT(*) as total_calls,
                   COUNT(CASE WHEN http_status = 200 THEN 1 END) as success_calls,
                   COUNT(CASE WHEN http_status != 200 THEN 1 END) as error_calls,
                   AVG(latency_ms) as avg_latency_ms,
                   MAX(latency_ms) as max_latency_ms,
                   MIN(latency_ms) as min_latency_ms
            FROM api_requests
            WHERE requested_at >= NOW() - INTERVAL '24 hours'
            GROUP BY source_id
            ORDER BY total_calls DESC
        """)
        
        print("\nüìä Th·ªëng k√™ API Calls (24h g·∫ßn nh·∫•t):")
        print("-" * 80)
        total_calls = 0
        total_time = 0
        
        for row in cur.fetchall():
            total_calls += row['total_calls']
            total_time += row['avg_latency_ms'] * row['total_calls'] / 1000  # seconds
            
            print(f"\n{row['source_id']}:")
            print(f"  T·ªïng calls: {row['total_calls']}")
            print(f"  Th√†nh c√¥ng: {row['success_calls']} ({row['success_calls']/row['total_calls']*100:.1f}%)")
            print(f"  L·ªói: {row['error_calls']}")
            print(f"  Latency trung b√¨nh: {row['avg_latency_ms']:.0f}ms")
            print(f"  Latency min/max: {row['min_latency_ms']:.0f}ms / {row['max_latency_ms']:.0f}ms")
        
        print(f"\nüìà T·ªïng c·ªông: {total_calls} calls trong ~{total_time:.1f}s")
    
    return total_calls, total_time


def analyze_ingestion_flow():
    """Ph√¢n t√≠ch flow ingestion v√† t√≠nh to√°n th·ªùi gian ∆∞·ªõc t√≠nh."""
    print("\n" + "=" * 80)
    print("PH√ÇN T√çCH INGESTION FLOW")
    print("=" * 80)
    
    # S·ªë l∆∞·ª£ng locations
    num_locations = 127  # 1 city + 126 wards
    
    # Rate limits
    rate_limits = {
        "openweather_onecall": {"per_minute": 60, "per_day": 1000},
        "openweather_air": {"per_minute": 60, "per_day": 1000},  # Shared v·ªõi One Call
        "openmeteo_weather": {"per_minute": None, "per_day": None},
        "openmeteo_air": {"per_minute": None, "per_day": None},
        "hanoiair": {"per_minute": None, "per_day": None}
    }
    
    # S·ªë calls per location
    calls_per_location = {
        "openweather_onecall": 1,  # 1 call cho current + hourly + daily
        "openweather_air": 2,  # 1 call current + 1 call forecast
        "openmeteo_weather": 1,
        "openmeteo_air": 1,
        "hanoiair": 0  # Ch·ªâ g·ªçi 1 l·∫ßn cho t·∫•t c·∫£ wards
    }
    
    # Latency ∆∞·ªõc t√≠nh (ms)
    estimated_latency = {
        "openweather_onecall": 800,
        "openweather_air": 900,
        "openmeteo_weather": 1200,
        "openmeteo_air": 1200,
        "hanoiair": 500
    }
    
    print("\nüìã C·∫•u h√¨nh Ingestion:")
    print(f"  S·ªë locations: {num_locations} (1 city + 126 wards)")
    
    print("\nüî¢ S·ªë l∆∞·ª£ng API Calls c·∫ßn thi·∫øt:")
    total_calls = 0
    for source, calls in calls_per_location.items():
        if source == "hanoiair":
            # HanoiAir: 1 call sync locations + 126 calls cho wards
            calls_needed = 1 + 126
        else:
            calls_needed = calls * num_locations
        
        total_calls += calls_needed
        print(f"  {source}: {calls_needed} calls")
    
    print(f"\n  T·ªîNG C·ªòNG: {total_calls} API calls")
    
    print("\n‚è±Ô∏è  Th·ªùi gian ∆∞·ªõc t√≠nh (kh√¥ng c√≥ rate limiting):")
    total_time_seconds = 0
    for source, calls in calls_per_location.items():
        if source == "hanoiair":
            calls_needed = 1 + 126
        else:
            calls_needed = calls * num_locations
        
        time_needed = (calls_needed * estimated_latency[source]) / 1000
        total_time_seconds += time_needed
        print(f"  {source}: {calls_needed} calls √ó {estimated_latency[source]}ms = {time_needed:.1f}s")
    
    print(f"\n  T·ªîNG TH·ªúI GIAN (l√Ω thuy·∫øt): {total_time_seconds:.1f}s (~{total_time_seconds/60:.1f} ph√∫t)")
    
    print("\nüö¶ ·∫¢nh h∆∞·ªüng c·ªßa Rate Limiting:")
    # OpenWeather: 60 calls/min, shared gi·ªØa 2 APIs
    openweather_calls = (calls_per_location["openweather_onecall"] + calls_per_location["openweather_air"]) * num_locations
    openweather_time = (openweather_calls / 60) * 60  # seconds (v·ªõi rate limit 60/min)
    print(f"  OpenWeather APIs: {openweather_calls} calls")
    print(f"    Rate limit: 60 calls/min (shared)")
    print(f"    Th·ªùi gian t·ªëi thi·ªÉu: {openweather_time:.0f}s (~{openweather_time/60:.1f} ph√∫t)")
    
    # Open-Meteo: kh√¥ng c√≥ limit
    openmeteo_calls = (calls_per_location["openmeteo_weather"] + calls_per_location["openmeteo_air"]) * num_locations
    openmeteo_time = (openmeteo_calls * estimated_latency["openmeteo_weather"]) / 1000
    print(f"  Open-Meteo APIs: {openmeteo_calls} calls")
    print(f"    Kh√¥ng c√≥ rate limit")
    print(f"    Th·ªùi gian ∆∞·ªõc t√≠nh: {openmeteo_time:.0f}s (~{openmeteo_time/60:.1f} ph√∫t)")
    
    # HanoiAir: 126 calls v·ªõi delay 0.5s
    hanoiair_time = 127 * 0.5 + (127 * estimated_latency["hanoiair"]) / 1000
    print(f"  HanoiAir: 127 calls (1 sync + 126 wards)")
    print(f"    Delay: 0.5s gi·ªØa c√°c calls")
    print(f"    Th·ªùi gian ∆∞·ªõc t√≠nh: {hanoiair_time:.0f}s (~{hanoiair_time/60:.1f} ph√∫t)")
    
    total_with_rate_limit = openweather_time + openmeteo_time + hanoiair_time
    print(f"\n  T·ªîNG TH·ªúI GIAN (v·ªõi rate limiting): {total_with_rate_limit:.0f}s (~{total_with_rate_limit/60:.1f} ph√∫t)")
    
    return {
        "total_calls": total_calls,
        "total_time_seconds": total_time_seconds,
        "total_with_rate_limit": total_with_rate_limit,
        "openweather_calls": openweather_calls,
        "openmeteo_calls": openmeteo_calls
    }


def analyze_current_performance(db_conn):
    """Ph√¢n t√≠ch hi·ªáu su·∫•t th·ª±c t·∫ø t·ª´ database."""
    print("\n" + "=" * 80)
    print("PH√ÇN T√çCH HI·ªÜU SU·∫§T TH·ª∞C T·∫æ")
    print("=" * 80)
    
    with db_conn.cursor(cursor_factory=RealDictCursor) as cur:
        # Th·ªëng k√™ theo location
        cur.execute("""
            SELECT 
                COUNT(DISTINCT location_id) as num_locations,
                COUNT(*) as total_records,
                COUNT(DISTINCT source_id) as num_sources
            FROM observations_raw
            WHERE created_at >= NOW() - INTERVAL '24 hours'
        """)
        obs_stats = cur.fetchone()
        
        cur.execute("""
            SELECT 
                COUNT(DISTINCT location_id) as num_locations,
                COUNT(*) as total_records,
                COUNT(DISTINCT source_id) as num_sources
            FROM forecasts_raw
            WHERE created_at >= NOW() - INTERVAL '24 hours'
        """)
        forecast_stats = cur.fetchone()
        
        print("\nüìä D·ªØ li·ªáu ƒë√£ ingest (24h g·∫ßn nh·∫•t):")
        print(f"  Observations: {obs_stats['total_records']} records t·ª´ {obs_stats['num_sources']} sources")
        print(f"    Locations: {obs_stats['num_locations']}")
        print(f"  Forecasts: {forecast_stats['total_records']} records t·ª´ {forecast_stats['num_sources']} sources")
        print(f"    Locations: {forecast_stats['num_locations']}")
        
        # Th·ªëng k√™ theo source v√† location
        cur.execute("""
            SELECT source_id, COUNT(DISTINCT location_id) as locations_ingested
            FROM (
                SELECT DISTINCT source_id, location_id FROM observations_raw
                WHERE created_at >= NOW() - INTERVAL '24 hours'
                UNION
                SELECT DISTINCT source_id, location_id FROM forecasts_raw
                WHERE created_at >= NOW() - INTERVAL '24 hours'
            ) AS combined
            GROUP BY source_id
            ORDER BY locations_ingested DESC
        """)
        
        print("\nüìç S·ªë locations ƒë√£ ingest theo source:")
        for row in cur.fetchall():
            print(f"  {row['source_id']}: {row['locations_ingested']} locations")


def suggest_optimizations(stats):
    """ƒê·ªÅ xu·∫•t t·ªëi ∆∞u h√≥a."""
    print("\n" + "=" * 80)
    print("ƒê·ªÄ XU·∫§T T·ªêI ∆ØU H√ìA")
    print("=" * 80)
    
    print("\nüí° C√°c ph∆∞∆°ng √°n t·ªëi ∆∞u:")
    
    print("\n1. **Parallel Processing (ƒêa lu·ªìng)**")
    print("   - Ch·∫°y ingestion cho nhi·ªÅu locations song song")
    print("   - S·ª≠ d·ª•ng ThreadPoolExecutor ho·∫∑c asyncio")
    print("   - Gi·∫£m th·ªùi gian t·ª´ ~20 ph√∫t xu·ªëng ~5-7 ph√∫t")
    print("   - ‚ö†Ô∏è  C·∫ßn c·∫©n th·∫≠n v·ªõi rate limits")
    
    print("\n2. **Batch Processing cho Open-Meteo**")
    print("   - Open-Meteo kh√¥ng c√≥ rate limit")
    print("   - C√≥ th·ªÉ ch·∫°y song song nhi·ªÅu requests")
    print("   - Gi·∫£m th·ªùi gian Open-Meteo t·ª´ ~4 ph√∫t xu·ªëng ~30 gi√¢y")
    
    print("\n3. **T·ªëi ∆∞u Rate Limiting**")
    print("   - OpenWeather: 60 calls/min (shared)")
    print("   - V·ªõi 127 locations √ó 2 APIs = 254 calls")
    print("   - C·∫ßn ~4.2 ph√∫t t·ªëi thi·ªÉu (kh√¥ng th·ªÉ gi·∫£m)")
    print("   - ‚úÖ ƒê√£ t·ªëi ∆∞u: rate limiter t·ª± ƒë·ªông wait")
    
    print("\n4. **Skip Redundant Calls**")
    print("   - Ki·ªÉm tra d·ªØ li·ªáu ƒë√£ c√≥ trong DB tr∆∞·ªõc khi g·ªçi API")
    print("   - Ch·ªâ update khi d·ªØ li·ªáu c≈© h∆°n freshness window")
    print("   - Gi·∫£m s·ªë l∆∞·ª£ng calls kh√¥ng c·∫ßn thi·∫øt")
    
    print("\n5. **HanoiAir Ward Forecasts**")
    print("   - Hi·ªán t·∫°i: 126 calls tu·∫ßn t·ª± v·ªõi delay 0.5s")
    print("   - C√≥ th·ªÉ ch·∫°y song song (kh√¥ng c√≥ rate limit)")
    print("   - Gi·∫£m t·ª´ ~2 ph√∫t xu·ªëng ~10-15 gi√¢y")
    
    print("\nüìä ∆Ø·ªõc t√≠nh th·ªùi gian sau t·ªëi ∆∞u:")
    print("   - OpenWeather: ~4.2 ph√∫t (kh√¥ng th·ªÉ gi·∫£m do rate limit)")
    print("   - Open-Meteo: ~30 gi√¢y (parallel)")
    print("   - HanoiAir: ~15 gi√¢y (parallel)")
    print("   - Data Fusion: ~30 gi√¢y")
    print("   - T·ªîNG: ~6 ph√∫t (thay v√¨ ~20 ph√∫t)")


def main():
    """Main function."""
    print("=" * 80)
    print("B√ÅO C√ÅO PH√ÇN T√çCH HI·ªÜU SU·∫§T INGESTION PIPELINE")
    print("=" * 80)
    print(f"Th·ªùi gian: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Ph√¢n t√≠ch flow
    stats = analyze_ingestion_flow()
    
    # Ph√¢n t√≠ch hi·ªáu su·∫•t th·ª±c t·∫ø
    try:
        db_conn = get_db_connection()
        analyze_current_performance(db_conn)
        analyze_api_calls(db_conn)
        db_conn.close()
    except Exception as e:
        logger.warning(f"Kh√¥ng th·ªÉ k·∫øt n·ªëi database: {e}")
    
    # ƒê·ªÅ xu·∫•t t·ªëi ∆∞u
    suggest_optimizations(stats)
    
    print("\n" + "=" * 80)
    print("K·∫æT LU·∫¨N")
    print("=" * 80)
    print(f"""
V·ªõi 127 locations, ingestion pipeline hi·ªán t·∫°i c·∫ßn:
- T·ªïng s·ªë API calls: {stats['total_calls']}
- Th·ªùi gian ∆∞·ªõc t√≠nh: ~{stats['total_with_rate_limit']/60:.1f} ph√∫t

Nguy√™n nh√¢n ch·∫≠m:
1. Rate limiting c·ªßa OpenWeather (60 calls/min) ‚Üí ~4.2 ph√∫t b·∫Øt bu·ªôc
2. Ch·∫°y tu·∫ßn t·ª± (sequential) ‚Üí kh√¥ng t·∫≠n d·ª•ng ƒë∆∞·ª£c parallel processing
3. Delay 0.5s gi·ªØa c√°c HanoiAir calls ‚Üí ~1 ph√∫t kh√¥ng c·∫ßn thi·∫øt

Gi·∫£i ph√°p:
- Implement parallel processing cho Open-Meteo v√† HanoiAir
- Gi·ªØ nguy√™n rate limiting cho OpenWeather (ƒë√£ t·ªëi ∆∞u)
- C√≥ th·ªÉ gi·∫£m th·ªùi gian t·ª´ ~20 ph√∫t xu·ªëng ~6 ph√∫t
    """)
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
